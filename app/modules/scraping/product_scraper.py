# app/product_scraper.py - Scraper y cat√°logo de productos PEISA
import json
import requests
from bs4 import BeautifulSoup
from typing import List, Dict
import re
import time

def scrape_peisa_products() -> List[Dict]:
    """
    Realiza scraping REAL del sitio web de PEISA para obtener productos actualizados.
    URL: https://peisa.com.ar/productos
    
    Extrae:
    - Categor√≠a (h1): ej. "Calderas centrales"
    - Subcategor√≠a (texto rojo): ej. "De potencia"
    - Productos con nombre, tipo y descripci√≥n
    - Ficha t√©cnica completa de cada producto
    
    Returns:
        Lista de productos extra√≠dos del sitio web
    """
    print("üåê Iniciando scraping de PEISA...")
    products = []
    
    try:
        # URL base de productos PEISA
        base_url = "https://peisa.com.ar"
        products_url = f"{base_url}/productos"
        
        # Headers para simular navegador
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        print(f"  üì° Conectando a {products_url}...")
        response = requests.get(products_url, headers=headers, timeout=10)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        print("  ‚úÖ P√°gina cargada correctamente")
        
        # Buscar todos los productos en la p√°gina
        # Estructura: <a href="/productos/[nombre]"><article>...</article></a>
        all_product_links = soup.find_all('a', href=re.compile(r'/productos/[^/]+$'))
        all_product_cards = [link for link in all_product_links if link.find('article')]
        
        print(f"  üîç Total de productos encontrados en la p√°gina: {len(all_product_cards)}")
        
        # Extraer categor√≠as para organizar
        categories = soup.find_all('h1')
        current_category = "Sin categor√≠a"
        current_subcategory = ""
        
        for category_elem in categories:
            category_name = category_elem.get_text(strip=True)
            
            # Buscar subcategor√≠a (texto rojo despu√©s del h1)
            subcategory_elem = category_elem.find_next_sibling()
            subcategory = ""
            if subcategory_elem and 'text-peisared' in str(subcategory_elem.get('class', [])):
                subcategory = subcategory_elem.get_text(strip=True)
            
            print(f"\n  üìÇ Categor√≠a: {category_name}")
            if subcategory:
                print(f"     ‚îî‚îÄ Subcategor√≠a: {subcategory}")
        
        # Procesar todos los productos encontrados
        print(f"\n  üì¶ Procesando {len(all_product_cards)} productos...")
        
        for idx, card in enumerate(all_product_cards, 1):
            try:
                # Intentar determinar categor√≠a del producto buscando el h1 m√°s cercano antes del producto
                product_category = "Sin categor√≠a"
                product_subcategory = ""
                
                # Buscar hacia atr√°s en el HTML para encontrar el h1 m√°s cercano
                prev_elem = card
                while prev_elem:
                    prev_elem = prev_elem.find_previous(['h1', 'p'])
                    if prev_elem and prev_elem.name == 'h1':
                        product_category = prev_elem.get_text(strip=True)
                        # Buscar subcategor√≠a despu√©s del h1
                        next_elem = prev_elem.find_next_sibling()
                        if next_elem and 'text-peisared' in str(next_elem.get('class', [])):
                            product_subcategory = next_elem.get_text(strip=True)
                        break
                
                # Extraer informaci√≥n b√°sica del producto
                product = extract_product_info(card, base_url, product_category, product_subcategory)
                
                if product and product.get('model'):
                    # Extraer caracter√≠sticas t√©cnicas y ventajas de la p√°gina de detalle
                    product_url = product.get('url')
                    if product_url:
                        technical_data = scrape_product_detail(product_url, headers)
                        product.update(technical_data)
                    
                    products.append(product)
                    print(f"  ‚úì [{idx}/{len(all_product_cards)}] {product['model']} - {product_category}")
                    
                # Peque√±a pausa para no sobrecargar el servidor
                time.sleep(0.3)
                    
            except Exception as e:
                print(f"  ‚ö†Ô∏è  Error procesando producto {idx}: {e}")
                continue
        
        print(f"\n‚úÖ Scraping completado: {len(products)} productos extra√≠dos")
        
    except requests.RequestException as e:
        print(f"‚ùå Error de conexi√≥n: {e}")
        print("‚ö†Ô∏è  No se pudo conectar al sitio web de PEISA")
        return []
    except Exception as e:
        print(f"‚ùå Error inesperado: {e}")
        import traceback
        traceback.print_exc()
        return []
    
    # Si no se encontraron productos, retornar lista vac√≠a
    if not products:
        print("‚ö†Ô∏è  No se encontraron productos en el sitio web")
        return []
    
    return products

def extract_product_info(card, base_url: str, category: str = "", subcategory: str = "") -> Dict:
    """
    Extrae informaci√≥n de un elemento HTML de producto de PEISA.
    
    Estructura esperada:
    <a href="/productos/[nombre]">
        <article>
            <div class="text-peisared-600">TIPO DE PRODUCTO</div>
            <h4>NOMBRE PRODUCTO</h4>
            <p>Descripci√≥n</p>
        </article>
    </a>
    
    Args:
        card: Elemento <a> que contiene el producto
        base_url: URL base del sitio
        category: Categor√≠a principal (h1) ej. "Calderas centrales"
        subcategory: Subcategor√≠a (texto rojo) ej. "De potencia"
        
    Returns:
        Diccionario con informaci√≥n del producto
    """
    product = {}
    
    try:
        article = card.find('article')
        if not article:
            return product
        
        # Extraer tipo de producto (texto en rojo dentro del article)
        # Ej: "CALDERA DE POTENCIA"
        type_elem = article.find('div', class_=re.compile(r'peisared|uppercase'))
        product_type = type_elem.get_text(strip=True) if type_elem else ""
        
        # Extraer nombre/modelo (h4)
        title_elem = article.find(['h4', 'h3', 'h2'])
        if title_elem:
            product['model'] = title_elem.get_text(strip=True)
        else:
            return product  # Sin nombre, no es v√°lido
        
        # Extraer descripci√≥n (p√°rrafo)
        desc_elem = article.find('p')
        if desc_elem:
            product['description'] = desc_elem.get_text(strip=True)
        else:
            product['description'] = f"{product_type} - {product['model']}"
        
        # Asignar categor√≠a y tipo
        product['category'] = category if category else "Sin categor√≠a"
        product['subcategory'] = subcategory if subcategory else ""
        product['type'] = product_type if product_type else determine_type(product.get('model', ''))
        
        # Determinar familia basada en categor√≠a principal
        if category:
            product['family'] = map_category_to_family(category)
        else:
            product['family'] = determine_family(product.get('model', ''))
        
        # URL del producto
        product_url = card.get('href', '')
        if product_url and not product_url.startswith('http'):
            product['url'] = base_url + product_url
        else:
            product['url'] = product_url
        
    except Exception as e:
        print(f"    Error extrayendo info: {e}")
    
    return product


def scrape_product_detail(product_url: str, headers: dict) -> Dict:
    """
    Extrae la ficha t√©cnica completa de un producto individual.
    
    Extrae:
    - Descripci√≥n completa del producto
    - Caracter√≠sticas t√©cnicas (lista de bullets)
    - Ventajas (lista de bullets)
    - Potencia, dimensiones, etc.
    
    Args:
        product_url: URL completa del producto
        headers: Headers HTTP para la petici√≥n
        
    Returns:
        Diccionario con datos t√©cnicos adicionales
    """
    technical_data = {
        'technical_features': [],
        'advantages': [],
        'specifications': {}
    }
    
    try:
        response = requests.get(product_url, headers=headers, timeout=10)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Extraer descripci√≥n completa del producto
        # La descripci√≥n est√° en el primer p√°rrafo despu√©s del subt√≠tulo (h2)
        # Buscar el h2 con el subt√≠tulo y luego el p√°rrafo siguiente
        subtitle = soup.find('h2')
        if subtitle:
            # Buscar el siguiente p√°rrafo despu√©s del h2
            next_p = subtitle.find_next('p')
            if next_p:
                desc_text = next_p.get_text(strip=True)
                # Separar la descripci√≥n de las ventajas si est√°n juntas
                # Buscar donde empieza "Ventajas" o "VentajasX" (sin espacio)
                if 'Ventajas' in desc_text:
                    # Dividir en la palabra "Ventajas"
                    desc_text = desc_text.split('Ventajas')[0].strip()
                
                # Verificar que sea una descripci√≥n v√°lida (m√°s de 50 caracteres)
                if len(desc_text) > 50:
                    technical_data['description'] = desc_text
        
        # Si no se encontr√≥ con el m√©todo anterior, buscar en todo el contenido
        if 'description' not in technical_data:
            # Buscar todos los p√°rrafos en el √°rea principal
            paragraphs = soup.find_all('p')
            for p in paragraphs:
                text = p.get_text(strip=True)
                
                # Separar la descripci√≥n de las ventajas si est√°n juntas
                if 'Ventajas' in text:
                    text = text.split('Ventajas')[0].strip()
                
                # La descripci√≥n es un p√°rrafo largo que no contiene palabras clave de secciones
                if (len(text) > 80 and 
                    'Caracter√≠sticas' not in text and
                    'garant√≠a' not in text and
                    'PUNTOS DE VENTA' not in text):
                    technical_data['description'] = text
                    break
        
        # Extraer ventajas (secci√≥n "Ventajas")
        # Primero intentar extraer de una lista <ul> despu√©s de un <h3>Ventajas</h3>
        ventajas_section = soup.find('h3', string=re.compile(r'Ventajas', re.I))
        if ventajas_section:
            ventajas_list = ventajas_section.find_next('ul')
            if ventajas_list:
                technical_data['advantages'] = [
                    li.get_text(strip=True) for li in ventajas_list.find_all('li')
                ]
        
        # Si no se encontraron ventajas en una lista <ul>, buscar en el texto del p√°rrafo
        if not technical_data['advantages']:
            # Buscar el p√°rrafo que contiene "Ventajas"
            subtitle = soup.find('h2')
            if subtitle:
                next_p = subtitle.find_next('p')
                if next_p:
                    full_text = next_p.get_text(strip=True)
                    # Si el p√°rrafo contiene "Ventajas", extraer la parte de ventajas
                    if 'Ventajas' in full_text:
                        ventajas_text = full_text.split('Ventajas', 1)[1] if 'Ventajas' in full_text else ''
                        if ventajas_text:
                            # Dividir por puntos seguidos de may√∫scula o por saltos de l√≠nea
                            ventajas_items = []
                            # Dividir por punto seguido de may√∫scula
                            parts = re.split(r'\.(?=[A-Z])', ventajas_text)
                            for part in parts:
                                part = part.strip()
                                if len(part) > 10:  # Filtrar fragmentos muy cortos
                                    # Agregar el punto final si no lo tiene
                                    if not part.endswith('.'):
                                        part += '.'
                                    ventajas_items.append(part)
                            
                            if ventajas_items:
                                technical_data['advantages'] = ventajas_items
        
        # Extraer caracter√≠sticas t√©cnicas (secci√≥n "Caracter√≠sticas T√©cnicas" o "Caracter√≠sticas t√©cnicas")
        # Buscar cualquier encabezado que contenga "caracter√≠sticas" y "t√©cnicas"
        caracteristicas_section = soup.find(['h2', 'h3', 'h4'], string=re.compile(r'Caracter√≠sticas\s+t√©cnicas', re.I))
        if caracteristicas_section:
            caracteristicas_list = caracteristicas_section.find_next('ul')
            if caracteristicas_list:
                technical_data['technical_features'] = [
                    li.get_text(strip=True) for li in caracteristicas_list.find_all('li')
                ]
        
        # Extraer especificaciones de cualquier lista de bullets visible
        all_lists = soup.find_all('ul')
        for ul in all_lists:
            items = [li.get_text(strip=True) for li in ul.find_all('li')]
            # Buscar especificaciones t√©cnicas en formato "Clave: Valor"
            for item in items:
                if ':' in item:
                    key, value = item.split(':', 1)
                    technical_data['specifications'][key.strip()] = value.strip()
        
        # Extraer potencia si est√° en especificaciones
        for key, value in technical_data['specifications'].items():
            if 'potencia' in key.lower():
                power_match = re.search(r'(\d+)\s*(w|kw|kcal)', value, re.I)
                if power_match:
                    power_value = int(power_match.group(1))
                    unit = power_match.group(2).lower()
                    if unit == 'kw':
                        power_value *= 1000
                    elif unit == 'kcal':
                        power_value = int(power_value * 1.163)
                    technical_data['power_w'] = power_value
        
        # Peque√±a pausa
        time.sleep(0.2)
        
    except Exception as e:
        print(f"         ‚ö†Ô∏è  Error obteniendo ficha t√©cnica: {e}")
    
    return technical_data


def map_category_to_family(category: str) -> str:
    """Mapea la categor√≠a de PEISA a nuestra familia de productos"""
    category_lower = category.lower()
    
    if 'caldera' in category_lower:
        return "Calderas"
    elif 'radiador' in category_lower or 'toallero' in category_lower:
        return "Radiadores"
    elif 'termotanque' in category_lower or 'tanque' in category_lower:
        return "Termotanques"
    elif 'calefon' in category_lower:
        return "Calefones"
    elif 'termostato' in category_lower:
        return "Termostatos"
    elif 'piscina' in category_lower or 'climatizador' in category_lower:
        return "Climatizaci√≥n Piscinas"
    elif 'detector' in category_lower:
        return "Seguridad"
    else:
        return "Otros"


def determine_family(model_name: str) -> str:
    """Determina la familia del producto seg√∫n su nombre"""
    model_lower = model_name.lower()
    
    if any(word in model_lower for word in ['radiador', 'broen', 'panel', 'toallero']):
        return "Radiadores"
    elif any(word in model_lower for word in ['caldera', 'boiler', 'prima', 'diva', 'summa']):
        return "Calderas"
    elif any(word in model_lower for word in ['piso', 'radiante', 'suelo']):
        return "Piso Radiante"
    elif any(word in model_lower for word in ['termotanque', 'tanque', 'agua caliente']):
        return "Termotanques"
    elif any(word in model_lower for word in ['bomba', 'circulador']):
        return "Accesorios"
    else:
        return "Otros"


def determine_type(model_name: str) -> str:
    """Determina el tipo espec√≠fico del producto"""
    model_lower = model_name.lower()
    
    if 'panel' in model_lower:
        return "Radiador de panel"
    elif 'toallero' in model_lower:
        return "Radiador toallero"
    elif 'caldera' in model_lower:
        return "Caldera mural"
    elif 'piso' in model_lower or 'radiante' in model_lower:
        return "Sistema completo"
    elif 'termotanque' in model_lower:
        return "Termotanque el√©ctrico"
    elif 'bomba' in model_lower:
        return "Bomba circuladora"
    else:
        return "Producto de calefacci√≥n"

def get_products_catalog(use_scraping: bool = True) -> List[Dict]:
    """
    Retorna el cat√°logo de productos PEISA mediante scraping.
    
    Args:
        use_scraping: Si True, intenta hacer scraping real del sitio web.
        
    Returns:
        Lista de productos extra√≠dos del sitio web
    """
    if use_scraping:
        return scrape_peisa_products()
    else:
        # Si no se quiere scraping, cargar desde archivo existente
        try:
            with open("data/products_catalog.json", "r", encoding="utf-8") as f:
                return json.load(f)
        except FileNotFoundError:
            print("‚ö†Ô∏è  Archivo products_catalog.json no encontrado. Ejecutando scraping...")
            return scrape_peisa_products()


def save_catalog(filename: str = "data/products_catalog.json", use_scraping: bool = True):
    """
    Guarda el cat√°logo en un archivo JSON, actualizando productos existentes.
    
    Args:
        filename: Ruta donde guardar el archivo
        use_scraping: Si True, intenta scraping real de PEISA
        
    Returns:
        Lista de productos guardados
    """
    import os
    os.makedirs(os.path.dirname(filename), exist_ok=True)
    
    print(f"\n{'='*60}")
    print(f"üöÄ SCRAPER DE PRODUCTOS PEISA")
    print(f"{'='*60}\n")
    
    # Cargar cat√°logo existente si existe
    existing_products = {}
    if os.path.exists(filename):
        try:
            with open(filename, 'r', encoding='utf-8') as f:
                existing_list = json.load(f)
                # Indexar por URL para b√∫squeda r√°pida
                existing_products = {p.get('url'): p for p in existing_list if p.get('url')}
                print(f"üìÇ Cat√°logo existente cargado: {len(existing_products)} productos")
        except Exception as e:
            print(f"‚ö†Ô∏è  Error cargando cat√°logo existente: {e}")
    
    # Obtener productos del scraping
    new_products = get_products_catalog(use_scraping=use_scraping)
    
    # Actualizar o agregar productos
    updated_count = 0
    added_count = 0
    
    for product in new_products:
        product_url = product.get('url')
        if product_url in existing_products:
            # Actualizar producto existente
            existing_products[product_url].update(product)
            updated_count += 1
        else:
            # Agregar nuevo producto
            existing_products[product_url] = product
            added_count += 1
    
    # Convertir diccionario de vuelta a lista
    final_products = list(existing_products.values())
    
    # Guardar en JSON
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(final_products, f, ensure_ascii=False, indent=2)
    
    print(f"\n{'='*60}")
    print(f"‚úÖ Cat√°logo guardado exitosamente")
    print(f"üìÅ Archivo: {filename}")
    print(f"üì¶ Total productos: {len(final_products)}")
    print(f"   ‚îú‚îÄ ‚ú® Nuevos: {added_count}")
    print(f"   ‚îî‚îÄ üîÑ Actualizados: {updated_count}")
    print(f"{'='*60}\n")
    
    return final_products


if __name__ == "__main__":
    import sys
    
    # Verificar si se pasa argumento --no-scraping
    use_scraping = "--no-scraping" not in sys.argv
    
    if use_scraping:
        print("üåê Modo: SCRAPING REAL de https://peisa.com.ar/productos")
        print("   - Actualiza productos existentes")
        print("   - Agrega productos nuevos")
    else:
        print("üìã Modo: Cargar desde archivo existente (sin scraping)")
    
    # Ejecutar scraping y guardar
    products = save_catalog(use_scraping=use_scraping)
    
    # Mostrar resumen
    print(f"\nüìä RESUMEN DEL CAT√ÅLOGO:")
    print(f"{'='*60}")
    
    # Agrupar por familia
    families = {}
    for p in products:
        family = p.get('family', 'Otros')
        families[family] = families.get(family, 0) + 1
    
    for family, count in sorted(families.items()):
        print(f"  ‚Ä¢ {family}: {count} productos")
    
    print(f"\nüìã PRODUCTOS EXTRA√çDOS:")
    print(f"{'='*60}")
    for idx, p in enumerate(products, 1):
        category = p.get('category', 'N/A')
        print(f"  {idx:2d}. {p['model']:<40} ({p['family']}) - {category}")
