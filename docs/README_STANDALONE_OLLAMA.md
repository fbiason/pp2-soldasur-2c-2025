# SOLDASUR 2025 - VersiÃ³n Standalone con Ollama

## ğŸ“‹ Resumen Ejecutivo

VersiÃ³n **standalone** del asistente inteligente Soldy para SOLDASUR S.A., completamente funcional en el navegador con integraciÃ³n local de **Ollama** para procesamiento de lenguaje natural.

**Fecha de actualizaciÃ³n:** 15 de Octubre, 2025  
**VersiÃ³n:** SOLDASUR 2025 v2.0 - Standalone  
**Estado:** âœ… Operativo con Ollama

---

## ğŸ¯ CaracterÃ­sticas Principales

### âœ¨ Funcionalidades
- **Sistema Experto Guiado**: Flujo paso a paso para cÃ¡lculo de calefacciÃ³n
- **Chat Libre con IA**: Consultas abiertas procesadas por Ollama (Llama 3.2)
- **CatÃ¡logo de Productos**: NavegaciÃ³n por categorÃ­as de productos PEISA
- **100% Local**: Sin dependencias de APIs externas o servicios en la nube
- **Sin Backend**: Funciona directamente desde archivos HTML/JS

### ğŸ¤– IntegraciÃ³n con Ollama
- **Modelo:** Llama 3.2 (3B parÃ¡metros)
- **Endpoint:** `http://localhost:11434/api/chat`
- **CaracterÃ­sticas:**
  - Procesamiento local y privado
  - Sin costos por uso
  - Respuestas rÃ¡pidas
  - Contexto de conversaciÃ³n mantenido
  - CatÃ¡logo de productos integrado en el prompt

---

## ğŸ“ Estructura de Archivos

```
app/
â”œâ”€â”€ soldasur2025.html          # PÃ¡gina principal standalone
â”œâ”€â”€ soldasur.js                # LÃ³gica del chatbot con Ollama
â”œâ”€â”€ soldasur.css               # Estilos del chatbot
â””â”€â”€ img/
    â”œâ”€â”€ soldy.png              # Avatar del chatbot (botÃ³n flotante)
    â””â”€â”€ soldy_head.png         # Imagen del chatbot (header)
```

---

## ğŸš€ InstalaciÃ³n y ConfiguraciÃ³n

### Requisitos Previos

1. **Ollama instalado**
   ```bash
   # Descargar desde: https://ollama.ai
   # O instalar con:
   curl -fsSL https://ollama.ai/install.sh | sh
   ```

2. **Modelo Llama 3.2 descargado**
   ```bash
   ollama pull llama3.2:3b
   ```

3. **Ollama expuesto a la red**
   - Abrir Ollama
   - Ir a Settings
   - Activar "Expose Ollama to the network"

### Verificar InstalaciÃ³n

```bash
# Verificar que Ollama estÃ¡ corriendo
ollama list

# DeberÃ­a mostrar:
# NAME              ID              SIZE      MODIFIED
# llama3.2:3b       a80c4f17acd5    2.0 GB    X days ago
```

---

## ğŸ’» Uso

### OpciÃ³n 1: Abrir directamente
1. Navegar a la carpeta `app/`
2. Abrir `soldasur2025.html` en el navegador
3. El chatbot estarÃ¡ disponible en la esquina inferior derecha

### OpciÃ³n 2: Servidor local (recomendado)
```bash
# Con Python
cd app
python -m http.server 8000

# Con Node.js
npx http-server app -p 8000

# Acceder a:
http://localhost:8000/soldasur2025.html
```

---

## ğŸ¨ Modos de InteracciÃ³n

### 1.  GuÃ­ame en un cÃ¡lculo
Flujo estructurado paso a paso:
1. Tipo de calefacciÃ³n (Piso radiante / Radiadores / Calderas)
2. Superficie en mÂ²
3. Zona geogrÃ¡fica (Norte / Centro / Sur)
4. Nivel de aislaciÃ³n (Buena / Regular / Mala)
5. **Resultado:** Carga tÃ©rmica + Productos recomendados

### 2. ğŸ’¬ Tengo una pregunta
Chat libre con Ollama:
- Consultas sobre productos
- InformaciÃ³n tÃ©cnica
- Comparaciones
- Recomendaciones personalizadas
- **Ollama procesa** con contexto del catÃ¡logo PEISA

### 3. ğŸ“¦ Buscar productos
NavegaciÃ³n por categorÃ­as:
- Calderas (11 productos)
- Radiadores (8 productos)
- Termotanques (3 productos)
- Calefones (2 productos)
- Toalleros (6 productos)
- Climatizadores (3 productos)
- Termostatos (2 productos)

---

## âš™ï¸ ConfiguraciÃ³n TÃ©cnica

### Variables en `soldasur.js`

```javascript
/* ConfiguraciÃ³n Ollama */
const OLLAMA_URL = 'http://localhost:11434/api/chat';
const OLLAMA_MODEL = 'llama3.2:3b';
```

### Personalizar Modelo

Para usar otro modelo de Ollama:

```javascript
// Opciones disponibles:
const OLLAMA_MODEL = 'llama3.2:3b';     // RÃ¡pido, 2GB
const OLLAMA_MODEL = 'llama3.1:8b';     // Balanceado, 4.7GB
const OLLAMA_MODEL = 'mistral:7b';      // Alternativa, 4.1GB
const OLLAMA_MODEL = 'gemma2:9b';       // Google, 5.4GB
```

### Ajustar ParÃ¡metros de IA

```javascript
options: {
    temperature: 0.7,        // Creatividad (0.0-1.0)
    num_predict: 500        // MÃ¡ximo de tokens
}
```

---

## ğŸ”§ PersonalizaciÃ³n

### Cambiar Productos

Editar el array `peisaProducts` en `soldasur.js`:

```javascript
const peisaProducts = [
    {
        model: "Nombre del Producto",
        family: "CategorÃ­a",
        category: "SubcategorÃ­a",
        description: "DescripciÃ³n breve",
        url: "https://url-del-producto.com"
    },
    // ... mÃ¡s productos
];
```

### Modificar System Prompt

En la funciÃ³n `callOllama()`, editar:

```javascript
const systemPrompt = `Eres un asistente experto en productos de calefacciÃ³n...
// Personalizar instrucciones aquÃ­
`;
```

### Cambiar Estilos

Editar `soldasur.css` para personalizar:
- Colores del chatbot
- TamaÃ±o de la ventana
- Animaciones
- TipografÃ­a

---

## ğŸ› SoluciÃ³n de Problemas

### Error: "Error en la API de Ollama"

**Causa:** Ollama no estÃ¡ corriendo o no estÃ¡ expuesto a la red

**SoluciÃ³n:**
1. Verificar que Ollama estÃ¡ activo:
   ```bash
   ollama list
   ```
2. Verificar configuraciÃ³n en Ollama Settings:
   - âœ… "Expose Ollama to the network" debe estar activado
3. Reiniciar Ollama

### Error: CORS

**Causa:** Restricciones de seguridad del navegador

**SoluciÃ³n:**
- Usar un servidor local (no abrir el HTML directamente)
- Ollama debe estar expuesto a la red

### Respuestas lentas

**Causa:** Modelo muy grande o hardware limitado

**SoluciÃ³n:**
1. Usar modelo mÃ¡s pequeÃ±o:
   ```bash
   ollama pull llama3.2:3b
   ```
2. Reducir `num_predict` en la configuraciÃ³n

### El chatbot no responde

**Verificar:**
1. Console del navegador (F12) para errores
2. Ollama estÃ¡ corriendo: `ollama list`
3. Puerto correcto: `http://localhost:11434`
4. Modelo descargado: `ollama list`

---

## ğŸ“Š Arquitectura TÃ©cnica

### Flujo de Datos

```
Usuario â†’ Frontend (HTML/JS)
           â†“
    ClasificaciÃ³n de IntenciÃ³n
           â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
    â†“             â†“
Sistema Experto   Ollama (Chat Libre)
    â†“             â†“
CÃ¡lculos +    Respuestas IA
Productos     + Productos
    â†“             â†“
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
           â†“
    Renderizado UI
```

### Componentes JavaScript

```javascript
// GestiÃ³n de estado
conversationHistory = []  // Historial para Ollama
conversationStep = 0      // Paso actual del flujo experto
userInputs = {}          // Variables capturadas
contextData = {}         // Contexto visible al usuario

// Funciones principales
startConversation()      // Inicia el chat
handleOptionClick()      // Procesa selecciones
handleChatInput()        // Procesa texto libre
callOllama()            // Llama a la API de Ollama
detectMentionedProducts() // Extrae productos mencionados
renderProducts()         // Muestra tarjetas de productos
```

---

## ğŸ” Privacidad y Seguridad

### Ventajas de Ollama Local

âœ… **100% Privado:** Los datos nunca salen de tu mÃ¡quina  
âœ… **Sin API Keys:** No se requieren credenciales externas  
âœ… **Offline:** Funciona sin conexiÃ³n a internet (despuÃ©s de descargar el modelo)  
âœ… **Sin LÃ­mites:** No hay restricciones de uso o cuotas  
âœ… **Cumplimiento:** Ideal para datos sensibles o regulaciones estrictas  

---

## ğŸ“ˆ Rendimiento

### Tiempos de Respuesta (Llama 3.2 3B)

| Hardware | Tiempo promedio |
|----------|----------------|
| CPU moderna | 2-5 segundos |
| GPU integrada | 1-3 segundos |
| GPU dedicada | 0.5-1 segundo |

### Consumo de Recursos

- **RAM:** ~4-6 GB (modelo + overhead)
- **Disco:** 2 GB (modelo)
- **CPU/GPU:** Variable segÃºn hardware

---

## ğŸš€ Mejoras Futuras

### Corto Plazo
- [ ] Streaming de respuestas (mostrar texto progresivamente)
- [ ] Historial de conversaciones persistente
- [ ] Exportar conversaciÃ³n a PDF
- [ ] Modo oscuro

### Medio Plazo
- [ ] Soporte para mÃºltiples modelos
- [ ] AnÃ¡lisis de sentimiento
- [ ] Recomendaciones personalizadas basadas en historial
- [ ] IntegraciÃ³n con sistema de cotizaciones

### Largo Plazo
- [ ] Fine-tuning del modelo con datos de SOLDASUR
- [ ] Multilenguaje (inglÃ©s, portuguÃ©s)
- [ ] IntegraciÃ³n con CRM
- [ ] Analytics de conversaciones

---

## ğŸ“ Soporte

### Recursos
- **DocumentaciÃ³n Ollama:** https://ollama.ai/docs
- **Modelos disponibles:** https://ollama.ai/library
- **Comunidad Ollama:** https://discord.gg/ollama

### Contacto
- **Proyecto:** SOLDASUR 2025
- **Desarrollador:** Franco Biason
- **Email:** franco.biason@gmail.com

---

## ğŸ“ Changelog

### v2.0 - Standalone con Ollama (15/10/2025)
- âœ… MigraciÃ³n completa a Ollama local
- âœ… EliminaciÃ³n de dependencias de APIs externas
- âœ… OptimizaciÃ³n de prompts para Llama 3.2
- âœ… Mejora en detecciÃ³n de productos mencionados
- âœ… ActualizaciÃ³n de documentaciÃ³n

### v1.0 - Sistema HÃ­brido (11/10/2025)
- âœ… UnificaciÃ³n de sistema experto y RAG
- âœ… Orquestador inteligente
- âœ… Interfaz unificada
- âœ… IntegraciÃ³n con backend Python

---

## ğŸ“„ Licencia

Este proyecto es propiedad de **SOLDASUR S.A.** y estÃ¡ desarrollado para uso interno y demostraciÃ³n.

---

## ğŸ‰ ConclusiÃ³n

El asistente Soldy ahora funciona completamente con **Ollama**, proporcionando:

âœ… **AutonomÃ­a total** - Sin dependencias externas  
âœ… **Privacidad garantizada** - Procesamiento 100% local  
âœ… **Costo cero** - Sin gastos por uso de APIs  
âœ… **Rendimiento Ã³ptimo** - Respuestas rÃ¡pidas con Llama 3.2 3B  
âœ… **Escalabilidad** - FÃ¡cil cambio de modelos segÃºn necesidades  

**Â¡Listo para producciÃ³n!** ğŸš€
